<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
 
            autoNumber: "all",
            formatNumber: function (n) {return +n}
      } 
  }
});
</script>

---
output: html_document
---

<img src="../Logo.png" style="position:absolute;top:10px;right:125px;width:250px;height=250px" />

### `r fileName="../Title.txt";readChar(fileName,file.info(fileName)$size)`
#### MCMC using Metropolis and Metropolis-Hastings Steps
#### `r format(Sys.Date(), format="%B %d, %Y")`

- - -

#### Table of Contents

[I. Introduction][]

[II.  Overview of the MCMC algorrithm][]

[III. The Metropolis step][]

[IV. The Metropolis-Hastings step][]

[V. A toolbox for Metropolis-Hastings][]

[The determinsitc model][]

[Data simulation][]

[Setup][]

[The proposal function][]

[The prior function][]

[The likeliood function][]

[The choose function][]

[A plotting function][]

[VI. Problem][]
```{r preliminaries, include = FALSE}
rm(list = ls())
library(knitr)
knitr::opts_chunk$set(cache = FALSE, warnings = FALSE, tidy = FALSE)

# uncomment out this to generate key
nokey = FALSE; key = TRUE

# uncomment out this to generate exercise
#nokey = TRUE; key = FALSE

set.seed(3)
```

<br>

#### I. Introduction

The purpose of this lab is to learn how Markov chain Monte Carlo (MCMC) algorithm approximate the marginal posterior distributions of parameters using accept-reject sampling implemented with Metropolis and Metropolis-Hastings steps. Accept-reject sampling is critical in ecology and social science because many of our most important models are non-linear. In these cases, Gibbs steps, which you used in the previous lab, will not work.

My goal in preparing this lab was to was to write math, code, and explanatory text that I wished I had when I struggled to learn this material. I have taught this lab a least a dozen times. One of the things I have learned is that students are usually poorly trained programmers and their inevitable programming errors interfere with a big picture, experiential understanding of MCMC. They get lost in the arcane details of debugging and wish that MCMC would just go away and leave them alone forever.

So, I am trying something different this time. I have provided a set of tested, general functions that can be easily modified for different problems and that provides a useful, instructional framework. The functions are not intended to be a bullet-proof package, but rather a general framework for implementing the MCMC sampling that can be tweaked (or crow-barred) to work for a broad range of problems. The functions in the toolbox provide a nice pedagogical construct for linking math, code, and concepts by creating a point of focus for different steps in Metropolis-Hastings algorithm using the light limitation of trees problem (from the likelihood lab) as an example.

As an added benefit, the functions offers a a reasonable illustration of structured programming. This is a powerful approach that systems engineers and professional programers use all the time. Basically, the idea is to break all problems into a series of relatively small, tested functions and then assemble the functions to complete the overall task. If you find yourself writing a hundred lines of code without structuring your work into functions, then you are almost guaranteed to encounter nasty bugs. Professionals do it differently for a reason. Structured code is less prone to errors, easier to debug, and more transparent.

For this lab to be successful, you must understand R functions and how they are invoked. The concept of arguments and how they work is absolutely critical. 

#### II. Overview of the MCMC algorrithm
We covered MCMC sampling in lecture and it would be good to have your notes open as you work through this handout. However the ideas are pretty simple. You need to begin by writing out the model and dissecting the full, model into a set of univariate, conditional distributions. The marginal posterior for each conditional distribution is estimated using the MCMC algorithm, as follows.

1. Set up a structure to store a “chain” of values for each parameter in a model. I like a list of arrays for this purpose but any multi-dimensional structure will do.

2. Pick a sensible starting value for each parameter and store it as the first element in each chain. 

3. Choose an approach for sampling from the posterior distribution of each unobseved quantity.  Exploit conjugate relationships whenever possible to allow you to sample *directly* from the posterior.  Otherwise use an accept-reject methods like Metropolis or Metropolis-Hastings

4. Choose one of the parameters in your model to be the first one estimated. Which you choose doesn't matter. Assuming that all of the other parameters are known and constant, make a random draw from the posterior distribution of the first parameter using its conditional distribution. Store the draw. Repeat this process for all of the parameters you seek to estimate, including those that quantify uncertainty.

5. When draws have been made from the posterior distribution of all parameters, go back to step 4). Repeat this process many times until the chains provide converged estimates of the shape of the posterior distribution. Estimates that have converged have the property that taking more samples would not change the posterior distribution, its mean, variance, kurtosis etc. Throw away the part of the chain that has not converged. 

6. Use the converged portions of the stored chains to make inferences by calculating moments, densities, quantiles etc.

####III. The Metropolis step

An MCMC algorithm is composed of a series of steps. In each step, we make a draw from the full conidtional distribution of an unobserved quantity, which is to say we make a draw assuming that all other unobserved quantities are known and constant. Many of these draws put together create a vector of each unobserved quantity. We then make inference using that vector (or vectors when we have multiple chains). You used Gibbs steps in the previoius lab to estimate a mean and a varince of normally distributed data exploiting conjugate relationships. Sometimes conjugates are not an option.  In this case you will need to use accept-reject samplling The Metropolis algorithm is one way to accomplish that. It proceeds as follows.  To simplify the language I will call all unobserved quantities "parameters", but remember these could be latent quantities or dervied quantities.

1. Define the chain for the parameter as $\textbf{x}$ of length $n$. The first element in the $\mathbf{x}$ vector ($x_{1}$) is known because we chose it. The choice doesn't matter much as long as we don't pick something that is clearly silly–for example a negative integer for a parameter representing a survival probability. 

2. Choose an new value $z$ from a proposal distribution. There are many ways to do this– we will learn one that works well and is easy to understand. We will define a proposal distribution, $q\left(z\vert x_{i},\sigma\right)$ as a probability density function for continuous parameters (a probability function for discrete parameters) that specifies the distribution the new value $z$ conditional on the current value of $x$ and a tuning parameter, $\sigma$. (For the Metropolis algorithm to work, these distributions must be symmetric – more about that in the next section). We choose the value of the tuning parameter to optimize the efficiency of the chain, but everything will still work (eventually) even if we make a sub-optimum choice (but not a ridiculous choice). How we choose values for $\sigma$ will be discussed later. So, the proposal distribution is centered on the current value–its mean, remembering, of course, that we may need to translate the mean and $\sigma$ into shape parameters appropriate for our choice of the type of proposal distribution. So the mean of the proposal distribution is $x_{i}$, and the parameter $\sigma$ determines how close the proposed, new value z is likely to be to $x_{i}$. 

![alt text](Proposal_distribution.gif)
Figure 1.  We make a draw of a proposed new value $z$ in the chain from a proposal distribution centered on the current value $x_i$.


3. Starting with the first value in the chain, we make a random draw from $q\left(z\vert x_{1},\sigma\right)$.

4. Now we want to decide, which should be kept in the chain (i.e., become $x_{2}$), the old value ($x_{1}$) or the shiny new one ($z$)? As explained in lecture, we do that with the ratio of the likelihoods multiplied by the priors  for each of the two alternatives, $x_{1}$ and $z$, $$R_1=\frac{[\textbf{y}|z][z]}{[\textbf{y}|x_{1}][x_1]}.$$ We keep $z$ (i.e, $x_{2}=z$) with probability $\min\left(R_{1},1\right)$ and we keep $x_{1}$ (i.e,. $x_{2}=x_{1}$) with probability $1-\min\left(R_{i},1\right)$. In practice (i.e. this lab) we will often use $R_i=\exp(\log([\textbf{y}|z][z]) - \log([\textbf{y}|x_{1}][x_1]))$.  Why? 

5. Increment $i$ and return to step 3. Repeat until a large number of samples have been chosen.

6. Use the samples collected after the chain has converged to make inferences about parameters and derived quantities of interest.


Students often ask, “Why is $z$ chosen probabilistically? Why is it possible to choose $z$ when $R_{i}<1$?” The reason is that failing to do so can mean that the chain gets “stuck” on a value of $x_{i}$ with high probability. This prevents us from learning about the shape of the distribution.

There are a couple of places it is easy to be confused. First, remember that $[\textbf{y}|z]$ and $[\textbf{y}| x_{i}]$ are likelihood functions using *all* of the data, just like the ones you programmed in the lab on likelihood – remember summing the log-likelihoods in a column. The likelihood functions may not be the same form as the proposal distribution, $q\left(z|x_{i}\right)$. Second, we usually need a model to “get the parameter into the likelihood”, that is, to make predictions that can be evaluated against data. So if our ecological model is $g(\mathbf{\theta})$, then the Metropolis acceptance ratio will be $R_{i}=\frac{[\textbf{y}|g(z)][z]}{[\textbf{y}|g(x_{i})][x_{i}]}$ where we are estimating one of the parameters in $\mathbf{\theta}$ and the others remain constant. If there are independent variables, these are implicitly included in $g(\mathbf{\theta})$.

#### IV. The Metropolis-Hastings step 
There is an important caveat about the Metropolis algorithm: the proposal distribution must be symmetric, which means that $q(z|x_{i},\sigma)=q(x_{i}|z,\sigma)$.This is a very simple idea which says that if the current value of the chain is $x_{i}$, the probability of drawing $z$ is equal to the probability of drawing $x_{i}$ if the chain is located at $z$. 

*Exercise*: Calculate the probability density random variable $z$=10, from a normal distribution with mean $\mu=x_{i}=12$ and standard deviation $\sigma=1.5$. Now calculate the density of a normally distributed random variable $x_{i}=12$ with mean $\mu=z=10$ and standard deviation $\sigma=1.5$ . Repeat this calculation for a gamma distribution (using moment matching, of course). Which distribution is symmetric? Think of another proposal distribution that is symmetric and one that is asymmetric. Could you use the Poisson for a proposal distribution for continuous parameters? What would be the problem with doing so? An important point–just because a distribution appears to be symmetric (shaped like a normal) doesn't mean that it is.  It must satisfy the equality $q(z|x_{i},\sigma)=q(x_{i}|z,\sigma)$

There are many cases when a symmetric proposal distribution works fine. But we run into problems if the distribution includes values that are not defined for the parameter, as will be the case with a normal proposal distribution if our parameter is strictly positive. In that case, we can make a draw from the normal, exponentiate it to keep it positive, and merrily use Metropolis.This is fondly known to students at CSU as the Hooten shuffle. It is one of his favorite tricks.  

Hastings (1970) showed that a very small adjustment to the calculation of $R_{i}$ relaxes the assumption that proposal distributions are symmetric. In the Metropolis-Hastings algorithm, we use $$R_{i}=\frac{[\textbf{y}|z][z]}{[\textbf{y}|x_{i}][x_{i}]}\cdot\frac{q\left(x_{i}|z,\sigma\right)}{q\left(z\vert x_{i},\sigma\right)}$$ and use the same decision process to decide on the value of x_{i+1}. Be sure that you understand that the quantity $\frac{q\left(x_{i}|z,\sigma\right)}{q\left(z\vert x_{i},\sigma\right)}$ is the ratio of the *probability densities* (or probabilities for discrete parameters) returned by the distribution $q$ and is not a new draw from the distributions. (This confusion has cause untold agony in past MCMC labs.)

I favor using Metropolis-Hastings for everything where Gibbs steps can't be used. When the proposal distributions are symmetric, $\frac{q\left(x_{i}|z,\sigma\right)}{q\left(z\vert x_{i},\sigma\right)}=1$, reducing the algorithm to Metropolis. By using Metropolis-Hastings for all proposal distributions, symmetric and asymmetric, I can have one set of code that applies in a general way to many problems, without having to worry about forgetting to include $\frac{q\left(x_{i}|z,\sigma\right)}{q\left(z\vert x_{i},\sigma\right)}$ for asymmetric distributions (another source of potential agony). Which brings us to the next topic, coding Metropolis-Hastings.

#### V. A toolbox for Metropolis-Hastings

I wrote some code for the light limitation of trees problem that you worked on in the likelihood lab. The deterministic model is
$$\mu_{i}=g(\alpha,\gamma,c,L_i)=\frac{\alpha(L_i-c)}{\frac{\alpha}{\gamma}+(L_i-c)}$$. 
We want to approximate the marginal posterior distributions of the parameters $\alpha,\gamma,c$, and $\sigma$, all of which are continuous and strictly positive. The growth rate is a real number that can be positive or negative. We have no prior knowledge of any of the parameters.Note that the light level for the $i^{th}$ tree is notated as $L_i$.  We are assuming it is observed witout error. The posterior and joint distribution is $$[\alpha,\gamma,c,\sigma|\textbf{y}]\propto\prod\limits _{i=1}^n[y_i|g(\alpha,\gamma,c,L_i,\sigma)[\alpha][\gamma][c][\sigma]$$

Before we get going, a bit of rationale for this exercise. You could estimate these parameters in a quick snap with one line of code in R using the `nls( )` function. Why bother with the pain of doing it by MCMC? The answer is that you can use MCMC for problems that would swiftly bring `nls()` to its knees. In the spirit of walking before running, we will learn MCMC using a simple model, but our learning will allow us to analyze much more complex ones that can be solved virtually no other way. Ok, Ok, there is also INLA but that is beyond the scope of the class. Moreover, as you will see, we will estimate the posterior distribution of some derived parameters, a impossible task with canned routines like `nls(`). Finally, the last time I checked, there was no easy way to get confidence bands on the predictions from `nls()`.

*Exercise*: What is a reasonable distribution to use for the likelihood? For the priors? For the proposal distribution?

*Exercise*: Write the full conditioal distributions for all of the parameters.

Study the code for the functions that follow.  Your challenge will be to use these functions to construct a MCMC algorithm using Metropolis-Hastings steps.


### The determinsitc model
Just to save you time:

```{r}
#Function for deterministic model
g=function(alpha, gamma,c, L){
	#the observed independet variable is w
	mu =alpha*(L - c) / (alpha/gamma + (L-c))
	return(mu)
}

```

### Data simulation

The function `get_data(alpha, gamma,c, sigma)` simulates data for a given set of parameter values, plots the data, and does a non-linear least squares fit. Why not use the real data that you used in the likelihood lab? Because you want to know the answer when you are learning new methods for obtaining answers. Always simulate your data.
```{r}
#Function to simulate data
get_data=function(alpha, gamma,c, sigma){
	set.seed(4)
	par(mfrow=c(1,1))
	L=sort(runif(50,min=12, max = 100))
	mu = g(alpha=alpha, gamma=gamma, c=c, L=L)
	plot(L,mu, typ="l")
	y=rgamma(length(mu), mu^2/sigma^2, mu/sigma^2)
	plot(L,y)
	lines(L,mu)

	model=nls(y ~ g(alpha=alpha,gamma=gamma,c=c, L=L), start=list(alpha=50,gamma=4,c=2))
	s=summary(model)
	p=coef(model)
	y.hat=g(alpha=p[1],gamma=p[2],c=p[3], L=L)

	lines(L,y.hat,col="red")
	legend(40,18, c("generating", "nls fit"), lty=c("solid", "solid"), col=c("black", "red"), bty="n")
	return(list(x=L, y=y, nls.alpha=p[1], nls.gamma=p[2], nls.c=p[3], nls.sigma = s$sigma, gen.alpha=alpha, gen.gamma=gamma, gen.c=c, gen.sigma=sigma))
}

```

### Setup
The function `setup=function(n.iter,n.chain,parameter.names, dim.x){ }` creates storage structures for the chains for each parameter, sets the first value in the chain, and sets the tuning parameters. The body of the function must be edited to choose initial values and tuning values. 
```{r}
#function to stetup storage for chains and name them--entries must be made in the function body
setup=function(n.iter,n.chain,parameter.names, dim.x){
	#set up storage for chains
	x=list()
	for(i in 1:length(parameter.names)) {
		x[[i]]=array(NA,dim=c(dim.x[i],n.iter,n.chain))
	}
	#assign parameter names to elements of the list
	names(x)=parameter.names
	#enter initial guesses at parameters here
	x$alpha[1,1,1]=60 
	x$c[1,1,1] = 10
	x$gamma[1,1,1]=3
	x$sigma[1,1,1]=5
	#enter tuning parameters here
	tune=list(
		alpha=10,
		c = 1,
		gamma=.3,
		sigma=2
	) #end of tune list
	x$tune = tune
	return(x)
} #end of setup function

```

It is called using the statement:

`x=setup(n.iter=n.iter,n.chain=1, parameter.names=c("alpha","c","gamma","sigma", "y.hat","growth_ratio"), dim.x=c(1,1,1,1,n,1))` 

So, the the calling statement includes arguments of the number of iterations (`n.iter`), the number of chains, a vector or parameter names, and a list of the dimensions of the parameters `("alpha","c","gamma","sigma")` and derived quantities `("y.hat","growth_ratio")`  *in the same order as the parameter names*. This list of dimensions is in place for the circumstance when we want to aprroximate a vector, which will often be the case. For example, the argument `n` gives the number of data points and the number of predictions in the vector, `y.hat`.

The function returns a named list (`x`) of vectors for each parameter as well as a list of the tuning parameters for each parameter. To show this structure, I took the liberty of setting n.iter = 10 to make it easy to see rather than the large number you would use (5000 or usually more). The structure x looks like this, omitting the bulky 50x10 matrix for y.hat:

```{r, eval=FALSE}
$alpha , , 1

60 NA NA NA NA NA NA NA NA NA

$c , , 1

10 NA NA NA NA NA NA NA NA NA

$gamma , , 1

3 NA NA NA NA NA NA NA NA NA

$sigma , , 1

5 NA NA NA NA NA NA NA NA NA

$tune 

$tune$alpha 10

$tune$c 1

$tune$gamma 0.3

$tune$sigma 2 

$support 

$support$alpha [1] "non-negative"

$support$c [1] "real"

$support$gamma [1] "non-negative"

$support$sigma [1] "non-negative" 

```

To understand this data structure, think about the parameter `alpha`, stored in `x$alpha`, which is an array with 1 row, `n.iter` columns, and 1 “sheet”. If we wanted 2 chains, there would be 2 “sheets” each consisting of 1 row and `n.iter` columns. The strength of this approach is that it allows vectors as well as scalars. So the prediction `y.hat` has 50 rows (one for each prediction), `n.iter` columns and 1 sheet. I assign NA's to all of the elements of the chain except the first. If we wanted to get the $i^{th}$ value of the chain for the parameter alpha, we use `x$alpha[1,i,1]` or equivalently, `x$alpha[i]`.How would we get the $i^{th}$ value of `sigma`? 

The setup function also sets initial values, tuning values (i.e., $\sigma$) for each of the parameters. These are guesses that can be changed. More about this in a moment. It also defines the type of support for each parameter, a critical step. So, if the parameter can take on negative or positive values, the support would be "real". The other possibilities are "non-negative" or "zero-one".

### The proposal function

The function `q()` returns 2 types of values depending on the argument, `type=`. If

`type=="draw"`

then the function returns a random draw from the gamma distribution with a mean centered on `mu` and a standard deviation equal to the tuning parameter. When we use the function in this way, we make `mu `= the current value in the chain . If 

`type=="density"`

then the function returns the density of theta, which allows us to give the function the value of the proposal or the current value in the chain. Take a look at how the function is called and think about the relationship between the function call and the function definition. 

There are also arguments to the function that determine what kind of proposal you want–a real number that can take on negative or positive values, a non-negative real number, or a number between 0-1. You choose these based on the characteristics of the parameter you are estimating.

```{r}
#Proposal function
q=function(theta,mu,tune, type){
	sigma=tune
	if (type == "density") return (dgamma(theta,mu^2/sigma^2, mu/sigma^2))
	if (type == "draw") return (rgamma(1,mu^2/sigma^2, mu/sigma^2))
}

```

### The prior function

The prior function provides flat, uniform priors on all of the parameters, returning probability densities on the log scale. You could make them informative by using the proper distribution and arguments. You could also choose other non-informative priors if you like by changing the distributions.

```{r}
#Function for priors.  Note that the densities are returned on the log scale.
prior=function(param,theta){  
	if(param == "alpha") return( dunif(theta, min=0, max=500, log=TRUE))
	if(param=="c") return(dunif(theta,min = 0, max = 200, log=TRUE))
	if(param=="gamma") return(dunif(theta,min=0,max=200,log=TRUE))
	if(param=="sigma" ) return(dunif(theta,min=0,max=200, log=TRUE))
	}
```

### The likeliood function
The likelihood function should resemble the one your wrote for the Bayes theorem lab. These should be getting quite familiar by now. There few things worth noting– you will need to modify this function if you want to use likelihood functions other than the normal, which will probably require the proper moment matching. Note the trick for preventing errors from infinite values, which can arise when a proposal produces a likelihood that is infinite or NaN. This can happen.  The function returns the log probability density of the ouput of the model conditional on all of the data.

```{r}
#Likelihood function	
Like=function(y,L,alpha,gamma,c ,sigma){
	mu=g(alpha=alpha, gamma=gamma,c=c, L=L)
	LogL = dnorm(y, mean=mu, sd=sigma, log=TRUE)
	return(sum(LogL[is.finite(LogL)]))
	}
```

### The choose function

The choose function calculates the ratio of the posterior draw for the current value and the proposed one and decides which one to keep using the algorithm we saw in lecture. Note expoentiating the difference between the logs of the likelihoods multiplied by the priors. Why is this a good idea rather than using the unlogged ratio?

```{r}
#Function to choose current or proposed value
choose=function(x, z, Like_z, Like_x, param, tune){
    numerator = Like_z + prior(param,theta=z)  # these are both logs so we add rather than multiply
    denominator = Like_x + prior(param,theta=x)
    q.ratio = q(theta=x,mu=z,tune=tune, type="density") / q(theta=z,mu=x,tune=tune, type="density")
    R =  exp(numerator - denominator) * q.ratio #because these are logs, we exponetiate the difference to ge the ratio.
    if(R > runif(1,min=0,max=1)) new =z
    else new = x
    	return(new)
}

```

Using the parameter `gamma` to illustrate, we call the `choose` function using
`choose(x=x$gamma[i-1], z=z, Like_z=Like_z, Like_x=Like_x, param="gamma", tune=tune$gamma`
Discuss where these argumenta come from with you colleagues. 


### A plotting function
To make life easier....

```{r}
do_plots=function(data, x, n.iter, burnin){
	
	#trace plots
	par(mfrow=c(2,2))
	plot(x$gamma,typ="l", xlab="Iteration"); abline(h=mean(x$gamma[burnin:n.iter]),col="red")
	plot(x$alpha,typ="l",, xlab="Iteration");abline(h = mean(x$alpha[burnin:n.iter]), col="red")
	plot(x$c,typ="l",, xlab="Iteration");abline(h = mean(x$c[burnin:n.iter]), col="red")
	plot(x$sigma,typ="l", xlab="Iteration"); abline(h=mean(x$sigma[burnin:n.iter]), col="red")

	par(mfrow=c(1,1))
	q.y.hat=apply(x$y.hat[,burnin:n.iter,1],1, function(x) quantile(x, c(.025,.5,.975)))
	plot(data$x,data$y,xlab="Light level", ylab="Growth rate", main="Prediction of growth rate")
	lines(data$x,q.y.hat[2,], col="orange", lwd="4")
	lines(data$x,q.y.hat[1,], lty = "dashed", col="orange" )
	lines(data$x,q.y.hat[3,], lty = "dashed", col="orange" )
	lines(data$x, g(alpha=data$nls.alpha, gamma=data$nls.gamma, c=data$nls.c, L=data$x), col="blue")
	legend(40,18, c("Median", "2.5% quantile", "97.5% quantile", "nlsfit"), lty=c("solid", "dashed", "dashed"),col=c("orange", "orange", "orange", "blue"),bty="n")


plot_density = function(p,v1,v2, param, burnin, n.iter){
	hist(p[burnin:n.iter],breaks=100, xlab="Value of parameter", freq=FALSE, main=param)
	abline(v=v1,col="red", lwd=2)
	abline(v=v2, col="blue", lwd=2)
	abline(v=median(p[burnin:n.iter]), col="orange", lwd=2)
}

par(mfrow=c(2,2))
plot_density(p=x$alpha,v1=data$gen.alpha, v2=data$nls.alpha, param=expression(alpha), burnin=burnin, n.iter=n.iter)
plot_density(p=x$gamma,v1=data$gen.gamma, v2=data$nls.gamma, param=expression(gamma), burnin=burnin, n.iter=n.iter)
plot_density(p=x$c,v1=data$gen.c, v2=data$nls.c, param=expression(c), burnin=burnin, n.iter=n.iter)
plot_density(p=x$sigma,v1=data$gen.sigma, v2=data$nls.sigma, param=expression(sigma), burnin=burnin, n.iter=n.iter)

} #end of plotting function

```

####VI. Problem
Use these functions and the R template to construct an MCMC algorithm for estimating the parameters and the derived quantities. I give you a stout dose of help by writing the code for the full conditional for $\alpha$. Your job is to fill in the code for $\gamma, c, \sigma$ and two derived quantities, the predicted value of the mean of $y$ for each value of $L$ and the ratio $\frac{\alpha}{\gamma}.  You must study the funcions and understand what each one does for this exercise to be meaningful. Y

A good way to work when writing algorithms like this one is to treat parameters as known (you know them because you used them to simulate the data), adding one unknown at a time.Y So you might usefully start by wrting blocks of code like the one for alpha, but use fixed arguments for the other unknowns.  Then "unfix" say, $\gamma$ leaving the others fixed.  Get that working.  Then "unfix" $\c$ etc. 

Your asnswer should resemble the following:

```{r echo=key, include=TRUE}
#Execute the function definitions.
source("Gibbs MH functions.R")
#Simulate data to resemble data use in likelihood lab
data=get_data(alpha=38.5, gamma=1.7, sigma=2, c=8)
n.iter=50000
x=setup(n.iter=n.iter,n.chain=1, parameter.names=c("alpha","c","gamma","sigma", "y.hat","growth_ratio"), dim.x=c(1,1,1,1,50,1))
tune=x$tune
  for( i in 2:n.iter){
    if(i%%1000==0) cat(i,"\n");flush.console() #iteration counter, prints every 1000th i
    #Update alpha
    #z = proposal(mu=x$alpha[i-1],tune=tune$alpha)
    z=q(mu=x$alpha[i-1],tune=tune$alpha, type="draw")
    Like_z = Like(y=data$y, L=data$x, alpha=z, c=x$c[i-1], gamma=x$gamma[i-1], sigma=x$sigma[i-1])
    Like_x = Like(y=data$y, L=data$x, alpha=x$alpha[i-1], c=x$c[i-1], gamma=x$gamma[i-1], sigma=x$sigma[i-1])
    x$alpha[i] =choose(x=x$alpha[i-1], z=z, Like_z=Like_z, Like_x=Like_x, param="alpha", tune=tune$alpha)
    
    # # #Update gamma
    #z = proposal(mu=x$gamma[i-1],tune=tune$gamma)
    z = q(mu=x$gamma[i-1],tune=tune$gamma, type="draw")
    Like_z = Like(y=data$y, L=data$x, alpha=x$alpha[i], c=x$c[i-1], gamma=z, sigma=x$sigma[i-1])
    Like_x = Like(y=data$y, L=data$x, alpha=x$alpha[i], c=x$c[i-1], gamma=x$gamma[i-1], sigma=x$sigma[i-1])
    x$gamma[i] =choose(x=x$gamma[i-1], z=z, Like_z=Like_z, Like_x=Like_x, param="gamma", tune=tune$gamma)
    
    # # #Update c
    #z = proposal(mu=x$c[i-1],tune=tune$c)
    z = q(mu=x$c[i-1],tune=tune$c, type="draw")
    Like_z = Like(y=data$y, L=data$x, alpha=x$alpha[i], c=z, gamma=x$gamma[i], sigma=x$sigma[i-1])
    Like_x = Like(y=data$y, L=data$x, alpha=x$alpha[i], c=x$c[i-1], gamma=x$gamma[i], sigma=x$sigma[i-1])
    x$c[i] =choose(x=x$c[i-1], z=z, Like_z=Like_z, Like_x=Like_x, param="c", tune=tune$c)
    
    # # #Update sigma
    #z = proposal(mu=x$sigma,tune=tune$sigma)
    z = q(mu=x$sigma[i-1],tune=tune$sigma, type="draw")
    Like_z = Like(y=data$y, L=data$x, alpha=x$alpha[i], c=x$c[i], gamma=x$gamma[i], sigma=z)
    Like_x = Like(y=data$y, L=data$x, alpha=x$alpha[i], c=x$c[i], gamma=x$gamma[i], sigma=x$sigma[i-1])    
    x$sigma[i] =choose(x=x$sigma[i-1], z=z, Like_z=Like_z, Like_x=Like_x, param="sigma", tune=tune$sigma)
    
    #estimate derived quantities
    x$y.hat[,i,1] = g(alpha=x$alpha[i],gamma=x$gamma[i],c=x$c[i],L=data$x)
    x$growth.ratio[i]= x$alpha[i] / x$gamma[i]
  } #end of iteration loop
  
#Set value for burin--the elements that will be discarded
burnin=15000
#Look at the output
do_plots(data=data, x=x, n.iter=n.iter, burnin=burnin)
	
```

Your output should look like:
