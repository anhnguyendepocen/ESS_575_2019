
<style>

/* uncomment out this to generate exercise */
# .hider {display: none;}  

# .hider2 {display: inline;} 

/* uncomment out this to generate key */
 .hider {display: inline;}  
 .hider2 {display: none;}  

</style>

---
output: html_document
---

<img src="../Logo.png" style="position:absolute;top:10px;right:125px;width:250px;height=250px" />

### `r fileName="../Title.txt";readChar(fileName,file.info(fileName)$size)`
#### Bayesian Model Selection
#### `r format(Sys.Date(), format="%B %d, %Y")`

- - -

#### Table of Contents

[Motivation][]

[Problem][]

[Matrix specification of linear models][]

[Literature Cited][]

```{r preliminaries, include = FALSE}
rm(list = ls())
library(knitr)
knitr::opts_chunk$set(cache = FALSE, warnings = FALSE, tidy = FALSE)

# uncomment out this to generate key
 nokey = FALSE; key = TRUE

# uncomment out this to generate exercise
 #nokey = TRUE; key = FALSE
```

<br>

####Motivation

In the first edition of their classic text "Bayesian Data Analysis", Gelman and colleagues wrote an entry in the index: "Model selection: why we don't do it."(Gelman et al. 1995).  The second edition (Gelman et al. 2004) relaxed this a bit by saying "Model selection: why we avoid it." Finally, the most recent addition (Gelman et al. 2013) reads "Model selection: why we reluctantly do it." This progression illustrates that model selection is becoming increasingly accepted as a tool in the Bayesian locker.

A reason that Gelman and colleagues express reluctance about model selection is simply because Bayesians believe in using prior knowledge in all analyses. One important way that knowledge can enter into a model is by prior information on the mechanistic role of predictor variables.  If we *know* that a predictor has a mechanistic influence on a response, why would we leave it out of a model (e.g., Hobbs et al. 2012, Ver Hoef 2015)? Other reasons for avoiding model selection are covered nicely in Gelman and Rubin 1995, Gelman and Shalizi 2013, and Ver Hoef 2015.

There are occasions, however, where model selection is needed by Bayesians. Widely used methods, the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), despite its name, are not Bayesian but instead are based on maximum likelihood.  In this exercise, you will learn to use Bayesian methods for model selection.  We would use the methods covered here if there were no out-of-sample data available for true model validation, the gold standard of model selection, or if computational limitations prevented the use of cross validation, the next best choice when out-of-sample data are not available. 

<br>

####Problem
You will model bird species richness in the 50 (actually 49) US states in response to a set of predictor variables, area, mean temperature, and mean precipitation.  Fit the following models and calculate the Deviance Information criterion (DIC), the Wantanabe-Akaike Information Criterion (WAIC), and Posterior Predictive Loss (Dsel) for each one. Use a prior mean of 0 and variance of 100 for each regression coefficient.

1. Model 1: Null model with only an intercept (no covariates).

2. Model 2: Intercept and area as covariate.

3. Model 3: Intercept and temperature as covariate.

4. Model 4: Intercept and precipitation as covariate.

5. Model 5: Intercept and area and temperature as covariates.

See the included file "Model selection math.pdf" for the mathematics standing behind each selection criterion.

<br>

####Matrix specification of linear models
Specifying linear models in matrix notation is compact and convenient relative to writing out the full model as a scalar equation when there are several predictor variables.  Consider the the typical, deterministic linear model with three coefficients:

$$ \mu_{i}=\beta_{0}+\beta_{1}x_{1i}+\beta_{2}x_{2i}.$$

It can be written in matrix form as 

$$\pmb{\mu}=\mathbf{X}\pmb{\beta},$$

where $\pmb{\beta}$ is a column vector $(\beta_0, \beta_1,\beta_3)$ with length = number of model coefficients, and $\mathbf{X}$ is a *design* matrix with the number of rows equal to the number of data points and the number of columns equal to the number of predictor variables + 1.  Column one contains all 1's (or 0's if you seek to force the intercept through 0).  Column two contains the data values of predictor variable 1,  column 2, predictor variable 2, and so on.  Of course, $\mathbf(y)$ is a vector of model output containing one prediction for each data point.  See the lecture notes covering matrix multiplication if you need some refreshing on how it works.

Statisticians use matrix notation exclusively to specify linear models, although their notation is not entirely consistent as you learned in lecture. The rest of us can benefit from becoming comfortable with matrix notation. It is particularly handy here because we can use a single JAGS file (well, two actually) to specify several different models using the code below. 


```{r eval=FALSE}
 z <- X %*% beta # the regression model in matrix form, returns a vector of length n
    for(i in 1:n)   { 
    lambda[i] <- exp(z[i])
    y[i] ~ dpois(lambda[i])
}
```

Note that `%*%` is the symbol for matrix multiplication in JAGS and R. 

The reason this is so handy is an R function, `model.matrix()`, for creating a design matrix, i.e., the $\mathbf(X)$.  Consider the following:

```{r eval=FALSE}
X = model.matrix(~as.numeric(scale(area)) + as.numeric(scale(temp)), data = bird.sm.df)
```
This creates a design matrix with 1's in column one and standardized data for area and temperature in columns two and three using the data in the data frame `bird.sm.df`. So you use matrix multiplication in the JAGS file and specify the model using a design matrix, which allows you to obviate the need for a different file of JAGS code for each model. Slick.

Your job is to 

1. Write an expression for the posterior and joint distribution.
2. Write code to fit the four models.
3. Conduct posterior predictive checks for each model.
4. Compute DIC, WAIC, and Dsel.

This will exercise your skill in summarizing jags objects using the rjags `summary( )` function.  I will give you the an answer below, but not the code to produce it, which will give you a way to check your code. Note that you will need to used separate code for the intercept only model. The models with slopes can be fit using the same code with different design matrices.

To check your answers, you should get very close to  DIC = 527, WAIC = 533, and Dsel = 65651 for model 5.  Means for the coefficients should be $\beta_0$ = 5.7, $\beta_1$= 0.91, $beta_2$ = 0.55.

Some preliminaries:
```{r}
####
####  Load Packages
####
library(rjags)
library(ESS575)
####
####  Load Data 
####

bird.df <- RichnessBirds

####
####  Remove Outliers 
####

idx.outlier=(1:51)[(bird.df$species==min(bird.df$species) | bird.df$area==max(bird.df$area))]
bird.sm.df=bird.df[-idx.outlier,]

####
####  Setup Data to Fit Model 
####A cool way to make a design matrix from a data frame.  Automatically makes the first column = 1 to allow for intercept.

#Use these to run differnt models, ucommenting one at a time.
#X = model.matrix(~as.numeric(scale(area)), data = bird.sm.df)
#X = model.matrix(~as.numeric(scale(temp)), data = bird.sm.df)
#X = model.matrix(~as.numeric(scale(precip)), data = bird.sm.df)
X = model.matrix(~as.numeric(scale(area)) + as.numeric(scale(temp)), data = bird.sm.df)
y = bird.sm.df$species  
M1.list <- list(y=y,X=as.matrix(X),n=length(y),p=dim(X)[2])
set.seed(7)
```



```{r,echo=key, include=key}
{
sink("pois.reg_nth")
cat("
model{
  z <- X %*% beta # the regression model in matrix form, returns a vector of length
  for(i in 1:n)   { 
    y[i] ~ dpois(lambda[i])
    lambda[i] <- exp(z[i])
    #calculate predicitve density for use in WAIC
    pd[i] <- dpois(y[i],lambda[i]) #note when the lhs of the <- is not data, dpois() returns a probability
    #calculate the log predicitve density for use in WAIC
    log_pd[i] <- log(dpois(y[i],lambda[i]))
    #simulate new data sets for posterior predictive loss
    y.new[i]~dpois(lambda[i])
  }
  
  
  # PRIORS
  # p = number of coefficients, including intercept
  for(i in 1:p) {  
    beta[i] ~ dnorm(0, 0.01)
  }
}
",fill=TRUE)
sink()
}
```

```{r, echo=key, include=key}
####  Fit Model Using JAGS 
####
set.seed(7)
#get DIC module for calculating deviance and DIC directly
load.module("dic")
inits=list(beta=c(mean(log(y)),rep(0,dim(X)[2]-1)))
M1.model <- jags.model("pois.reg_nth",data=M1.list,inits=inits,n.chains=2,n.adapt=1000)
update(M1.model, n.iter=3000)
M1.out <- coda.samples(M1.model,variable.names=c("beta","lambda"),n.iter=8000)
summary(M1.out)
zj=jags.samples(M1.model,data=M1.list, inits=inits, n.chains=2,variable.names=c("log_pd","pd","beta","lambda", "deviance", "y.new"), n.iter=10000)
zDIC=dic.samples(M1.model, n.iter=8000)


```




```{r, eval=key, include=key, echo=key}
{
sink("pois.reg.0_nth.R")
cat("
model {
  
  # LIKELIHOOD
  # n= number of states
  # y = number of birds in each state
  
  for(i in 1:n)   { 
    y[i] ~ dpois(lambda[i])
    z[i] <- beta
    lambda[i] <- exp(z[i])
    #calculate predicitve density for use in WAIC
    pd[i] <- dpois(y[i],lambda[i]) #note when the lhs of the <- is not data, dpois() returns a probability
    #calculate the log predicitve density for use in WAIC
    log_pd[i] <- log(dpois(y[i],lambda[i]))
    #simulate new data sets for posterior predictive loss
    y.new[i]~dpois(lambda[i])
  }
  
  
  # PRIOR

    beta ~ dnorm(0, 0.01)

  
}

    ",fill=TRUE)
sink()
}
```

```{r, eval=key, include=key, echo=key}
###
##Run these inits, M1.model, and mean commands for null model only
##
inits=list(beta=c(mean(log(y)))) # uncomment for the null model
M1 <- M1.list <- list(y=y,X=as.matrix(X),n=length(y),p=dim(X)[2])
M1.model <- jags.model("pois.reg.0_nth.R",data=M1.list,inits=inits,n.chains=2,n.adapt=1000)
update(M1.model,n.iter=3000)
M1.out <- coda.samples(M1.model,variable.names=c("beta","lambda"),n.iter=8000)
zj=jags.samples(M1.model,data=M1.list, inits=inits, n.chains=2,variable.names=c("log_pd","pd","beta","lambda", "deviance", "y.new"), n.iter=10000)
zDIC=dic.samples(M1.model, n.iter=8000)
```


```{r eval=key, echo=key, include=key}
#Model selection statistics.  All checked against Mevin's code
#DIC
mean.lambda = summary(zj$lambda,mean)$stat
Dhat = -2*(sum(dpois(y,mean.lambda,log=TRUE))) 
Dbar = summary(zj$deviance, mean)$stat
pD.DIC = Dbar - Dhat
DIC = Dhat + 2*pD.DIC
c(pD.DIC,DIC)
#Now use built in function from dic.samples( )  above
zDIC
###

####### WAIC

lppd=-2*sum(log(summary(zj$pd,mean)$stat))
pD.WAIC=sum((summary(zj$log_pd,sd)$stat)^2)
WAIC=lppd +2*pD.WAIC
c(pD.WAIC,WAIC)


######Posterior predictive loss

Dsel = sum((y-summary(zj$y.new,mean)$stat)^2) + sum((summary(zj$y.new,sd)$stat)^2)
Dsel
```

####Literature cited 

Gelman, A., and D. B. Rubin. 1995. Avoiding model selection in Bayesian social research. Pages 165-173  Sociological Methodology 1995, Vol 25.

Gelman, A., and C. R. Shalizi. 2013. Philosophy and the practice of Bayesian statistics. British Journal of Mathematical & Statistical Psychology 66:8-38.

Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin. 1995, 2004. Bayesian data analysis. Chapman and Hall / CRC, London.

A. Gelman, J. B. Carlin, H. S. Stern, D. Dunson, A. Vehhtari, and D. B. Rubin. 2013. Bayesian data analysis. Chapman and Hall / CRC, London, UK, 2013.

Hobbs, N. T., H. Andren, J. Persson, M. Aronsson, and G. Chapron. 2012. Native predators reduce harvest of reindeer by Sami pastoralists. Ecological Applications 22:1640-1654.

Hoef, J. M. V., and P. L. Boveng. 2015. Iterating on a single model is a viable alternative to multimodel inference. Journal of Wildlife Management 79:719-729.


