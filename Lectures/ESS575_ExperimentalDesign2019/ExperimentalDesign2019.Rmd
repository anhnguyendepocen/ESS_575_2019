---
title: "Bayesian Analysis of Designed Experiments"
author: "N. T. Hobbs and C. Che-Castaldo"
date: "April 16, 2019"
output: 
  beamer_presentation:
    includes:
      in_header: header.tex
theme: Boadilla
subtitle: ESS 575
latex_engine: xelatex
transition: fastest
---

## Analyzing experimental data: Why Bayes?

\centerline{\includegraphics[height=2.75in]{SokalandRolf.png}}


## Analyzing experimental data: Why Bayes?

\centerline{\includegraphics[height=1 in]{ExperimentalPlot.jpg}}
* Probabilistic interpretation of effects
* Contrasts easy to construct
* No "error" terms
* Can accommodate errors in responses (and predictors for ANCOVA)
* Can accommodate missing data
* Multiple comparisons of means handled sensibly
* Derived quantities handled easily

\vspace{3mm}
\tiny{Photo c/o of the Minnesota Agricultural Experiment Station at HTTP://WWW.meas.mun.ed.}

## Yellowstone experiment
\centerline{\includegraphics[height=3 in]{Height_comparison_at_elk.jpg}}

## Analysis of the joint distribution of data and parameters
\centerline{\includegraphics[height=1.65 in]{HeightOvertime.pdf}}
\vspace{-.4 in}
\begin{align}
\mu_{ijt}&=\beta_{0_{j}}+(\beta_{1}+\beta_{2}x_{1,ij}+\beta_{3}x_{2,ij}+\\&\beta_{4}x_{1,ij}x_{2,ij})t\\y_{ijt}&\sim\text{lognormal}(\log(\mu_{ijt}),\sigma_{j}^{2})\nonumber\\\beta_{0j}&\sim\text{normal}(\mu_{\beta_{0}},\sigma_{\beta_{0}}^{2})\nonumber\\\mu_{\beta_{0}}&\sim\text{normal}(0,10000)\nonumber\\\sigma_{\beta_{0}}^{2}&\sim\text{uniform}(0,5)\\\beta_{i\in1,...,3}&\sim\text{normal}(0,10000)
\end{align}



## Leanring objectives
* Understand alternative notation for models of designed experiments.
* Be able to compose Bayesian models for simple experimental designs.
* Build a foundation of knowledge needed for developing models appropriate for your specific research.


## Toics
* Review of matrix algebra
* Alternative notation
    - Design matrix
    - Indexed parameters
* Specifying models including design
    - Completely random
    - Randomized complete block
    - Split plot
* A general approach model building
    - Means models vs effects models
    - Constraints on parameters
* Inference
    - Effect sizes
    - Contrasts
    - Multiple comparisons
    

## Matrix notation for linear models

Remember matrix multiplication?

Example of matrix multiplication for $n$ observations using 2 predictor variables $x_{i,1}$ and $x_{i,2}$ and an intercept. 

$$\begin{aligned}
\left(\begin{array}{ccc} 1 & x_{1,1} & x_{1,2}\\ 1 & x_{2,1} & x_{2,2}\\ 1 & x_{3,1} & x_{3,2}\\ 1 & . & .\\ 1 & . & .\\ 1 & . & .\\ 1 & x_{n,1} & x_{n,2} \end{array}\right)\left(\begin{array}{c} \beta_{0}\\ \beta_{1}\\ \beta_{2} \end{array}\right)&=&\left(\begin{array}{c} \beta_{0}+\beta_{1}x_{1,1}+\beta_{2}x_{1,2}\\ \beta_{0}+\beta_{1}x_{2,1}+\beta_{2}x_{2,2}\\ \beta_{0}+\beta_{1}x_{3,1}+\beta_{2}x_{3,2}\\ .\\ .\\ .\\ \beta_{0}+\beta_{1}x_{n,1}+\beta_{2}x_{n,2} \end{array}\right)=\left(\begin{array}{c} \mu_{1}\\ \mu_{2}\\ \mu_{3}\\ .\\ .\\ .\\ \mu_{n} \end{array}\right)
\end{aligned}$$


## Matrix notation for linear models

You will often see models written using something like
$$y_i \sim \text{normal}(\mathbf{x}_i'\boldsymbol{\beta},\sigma^2)$$
or
$$y_i \sim \text{normal}(\mathbf{x}_i^T\boldsymbol{\beta},\sigma^2)$$
or (incorrectly, in my view)
$$y_i \sim \text{normal}(\mathbf{X}_i\boldsymbol{\beta},\sigma^2)$$
or
$$\mathbf{y} \sim \text{multivariate normal}(\mathbf{X}\boldsymbol{\beta},\sigma^2I)$$
Note that $\mathbf X$ is a matrix with ones in column 1 and values of covariates in other columns. Thus, $\mathbf X \boldsymbol{\beta}$ returns a vector. 


## Matrix notation for linear models

You also see models written using something like
$$y_i \sim \text{normal}(\beta_0 + \mathbf{x}_i'\boldsymbol{\beta},\sigma^2)$$
or
$$y_i \sim \text{normal}(\beta_0 + \mathbf{x}_i^T\boldsymbol{\beta},\sigma^2)$$
Note that in this case $\mathbf X$ is a matrix of values of covariates in columns. It does not have a ones in column one. I like this form because it is easy to create multi-level models by simply subscripting $\beta_0$ to represent groups.  Often you see the$'$ or the $T$ superscript omitted.

## Design matrix: What is this?

\centerline{\includegraphics[height=2 in]{QuantitativeDesignMatrix.png}}

\vspace{5mm}
- Great! But how do we handle categorical predictor variables, i.e., different treatments and treatment levels in an experiment, qualitative variables in descriptive (non-experimental) models?
- Categorical = non-metric = nominal = qualitative


## Note

The next several slides pertain to completely random designs with controls, so that the $\beta_0$ term can be interpreted as the mean in the control or "reference" condition.  The slope terms represent "effects", the changes in the control attributable to different treatments and treatment levels. These are called "effects models."  We will talk about more complex designs, "means models", and models that lack controls subsequently. 



## Notation for models: design matrices

\centerline{\includegraphics[height=2.5in]{DesignMatrix.png}}

## Notation for models: design matrices with repeated measures

\centerline{\includegraphics[height=2.5in]{DesignMatrixRepeatedMeasures.png}}

## Notation for models: design matrices with interactions

\centerline{\includegraphics[height=2.5in]{DesignMatrixInteractions.pdf}}

## Notation for models: subscripting parameters

\centerline{\includegraphics[height=1.85in]{CoefficientMatrix.png}}

## Notation for models: subscripting parameters with interactions

\centerline{\includegraphics[height=1.85in]{CoefficientMatrixInteractions.png}}

## Which notation to use?

* Design matrix
    - Clear interpretation analogous to regression
    - Coefficients drop out for controls because $\beta x_i=0$
    - Easy to include quantitative covariates
    - Easily adapted for multiple models using R's `model.matrix()` function
    - Interactions easy to interpret
    - Particularly well suited to single levels of treatment with control
* Subscripting parameters
    - Easier to write for complex models
    - Most models in texts written this way
    - Somewhat easier to code
    - Must exercise care interpreting coefficients
    
## Recall ignorability and reserach design

* Designs that are ignorable require no indices other that an index for the individual observations (i.e. $y_i$ ) and the covariates ($x_i$). Simple random sampling and completely randomized experiments have ignorable designs. In these cases $[I\mid x, y, \phi] = [I]$.

* Designs that are not completely random, for example, randomized complete block experiments, stratified random sampling, cluster, and others are not ignorable and must include information on the design in the analysis. Usually, proper indexing and information about the sample sizes specifies all of the needed information. In these cases, $[ I\mid x, y, \phi] = [I|x]$.

## Widely used designs in ecology illustrated with Yellowstone treatments

\centerline{\includegraphics[height=2.5in]{DesignSketch.png}}

##
\begin{align}
&\text{Completely Random\nonumber}\\
\mu_{km}&=\beta_0+\beta_{1,k}+\beta_{2,m} +\beta_{3,km}\nonumber\\
y_{ikm}&\sim~\text{lognormal}(y_{ikm}\mid\log(\mu_{km}),\sigma^2)\nonumber\\
\beta_0&\sim\text{normal}(0,10000)\nonumber\\
\nonumber\\
&\text{Randomized Complete Block\nonumber}\\
\mu_{jkm}&=\beta_{0,j}+\beta_{1,k}+\beta_{2,m} +\beta_{3,km}\nonumber\\
\beta_{0,j}&\sim\text{normal}(\mu_{\beta_0},\sigma^2_{\beta_0})\nonumber\\
y_{ijkm}&\sim~\text{lognormal}(y_{ijkm}\mid\log(\mu_{jkm}),\sigma^2)\nonumber\\
\nonumber\\
&\text{Split Plot}\nonumber\\
\mu_{jkm}&=\beta_{0,j}+\beta_{1,k}+\beta_{2,m} +\beta_{3,km}\nonumber\\
\beta_{0,j}&\sim\text{normal}(\mu_{\beta_0},\sigma^2_{\beta_0})\nonumber\\
y_{ijkm}&\sim~\text{lognormal}(y_{ijkm}\mid\log(\mu_{jkm}),\sigma_{km}^2)\nonumber\\
\end{align}

####A general approach to building models with categorical predictors

## Choices in specifying models

\centerline{\includegraphics[height=2.75in]{Parameterizations.png}}

## Cell Means Model: Joint and DAG

\centerline{\includegraphics[height=1.5in]{CellMeansJointDAG.png}}

- Interest in group means and not effects
- Have prior information for group means
- Lack prior information for group means - use vague priors
- Number of parameters = number of unknowns 
- Recover effects or grand mean as derived quantities

## Cell Means Model: Design Matrix

\centerline{\includegraphics[height=1.5in]{CellMeansDesignMatrix.png}}

## Cell Means Model: JAGS

\tiny
```{r, include = TRUE, echo = TRUE, eval= FALSE}
#priors
for (i in 1:5) {
  alpha[i] ~ dnorm(0, 0.001)
 }
sigma ~ dunif(0, 100)
tau <- 1 / ( sigma * sigma)

# Likelihood
#Subscripted model
for (i in 1:50) {
  y[i] ~ dnorm(alpha[x[i]], tau) #x[i] is index, 1 - 5
}

# or, equivalently the likelihood could be
# mu = X %*% alpha #X is 50 x 6 design matrix
# for (i in 1:50) {
#   y[i] ~ dnorm(mu[i], tau) 
}

# Derived quantities
diff.2.1 <- alpha[2] - alpha[1]
diff.3.1 <- alpha[3] - alpha[1]
grandMean <- mean(alpha)
effects =  alpha - grandMean
```

\normalsize

- Use the index trick implements subscripted parameter model.
- Matrix multiplication implements design matrix model.
- Compute effects and contrast as derived quantities


## Effects Models - Set to Zero: Joint and DAG

\centerline{\includegraphics[height=1.35in]{EffectsModelJointDAG.png}}

- Interest in effects and not means
- Have prior information for effect sizes
- Lack prior information for effect sizes - can estimate conservatively
- Number of parameters > number of unknowns requires constraint
- Recover group means as derived quantities

## Effects Model- Set to Zero: Design Matrix

\centerline{\includegraphics[height=1.35in]{EffectsSetZeroDesignMatrix.png}}

\vspace{5mm}
- CRD with 1 factor and 5 levels
- Remove parameter by setting  $\alpha_{5}=0$
- Group 1 is now represented by intercept $\mu$
- $\alpha_{5}$ represent deviations from this baseline/control group

## Effects Models - Set to Zero: JAGS

\tiny
``` {r, eval=FALSE}
# Priors
for (i in 1:5){
  alpha[i] ~ dnorm(0, 0.001)
}
mu0 <- alpha[1]
sigma ~ dunif(0, 100)
tau <- 1 / ( sigma * sigma)

# Likelihood, design matrix model
mu = X %*% alpha  #X is design matrix with 1's in column one
for (i in 1:50) {
  y[i] ~ dnorm(mu[i] , tau) 
}
  #or, eqivalently
# for (i in 1:50) {
#   y[i] ~ dnorm(mu0 + alpha[x[i]], tau)  #x[i] is 1-4 index of treatment 
#   }


# Derived quantities
cell[1] <- alpha[1]
for (i in 2:4){
  cell[i] <- mu0 + alpha[i]
}
grandMean <- mean(cell[])
```

\normalsize

- Compute cell and grand means as derived quantities

## Parameterize a model with categorical predictors

\centerline{\includegraphics[height=2.75in]{Parameterizations.png}}

## Effects Model - Multi-level: Joint and DAG

\centerline{\includegraphics[height=2.05in]{EffectsModelMultiJointDAG.png}}

- Interest in effects and not means
- Have prior information for effect sizes
- Lack prior information for effect sizes - can estimate conservatively
- Number of parameters > number of unknowns is ok! Why?
- Recover group means as derived quantities

## Effects Model - Multi-Level: Design Matrix

\centerline{\includegraphics[height=1.5in]{EffectsMultiDesignMatrix.png}}

- CRD with 1 factor and 5 levels
- Intercept, $\mu$, is the grand mean
- $\alpha_{j}$ represent deviations from the grand mean
- $\alpha_{j}$ are partially pooled allowing us to estimate all of them directly

## Effects Models - Multi-level: JAGS

\tiny
``` {r, eval=FALSE}
# Priors
mu0 ~ dnorm(0, 0.001)
for (i in 1:2){
  sigma[i] ~ dunif(0, 100)
  tau[i] <- 1 / ( sigma[i] * sigma[i])
}

# Likelihood
for (i in 1:6){			
  alpha[i] ~ dnorm (0, tau[2])
}
mu0 <- alpha[1]
#Design matrix approach
mu[i]  = X %*alpha
for (i in 1:50) {
  y[i] ~ dnorm(mu[i], tau[1]) 
}

# #Subscript approach
# for (i in 1:50) {
#   y[i] ~ dnorm(mu0 + alpha[x[i]], tau[1]) #x contains indexes 2-6
# }

# Derived quantities
for (i in 2:6){			
  cell[i] <- mu0 + alpha[i] 
}
```

\normalsize

- Use index trick for subscript model.
- Compute cell means as derived quantities


#### Inference

## Clairity of interpretation

\begin{columns}[T] % contents are top vertically aligned
\begin{column}[T]{6cm} % each column can also be its own environment
\begin{itemize}
\item In contrast, we make infernce on marginal posteriors.
\item "The probability that the effect of treatment exceed 0 was .95"
\item "We can be 90\% certain that the dam treatment doubled willow height by year 17."
\item Pr(Browsed > Unbrowsed) = .83
\item CI95: effect of browse = -4.0%, 3.7%
\end{itemize}

\end{column}
\begin{column}[T]{5cm} % alternative top-align that's better for graphics
\centerline{\includegraphics[height=2.25in]{p_values.png}}
\end{column}
\end{columns}
\vspace{8mm}

## Bayesian ANOVA

A way to summarize the "relative importance of different sources of variation in a dataset." \tiny(Gelman and Hill, 2007)

\normalsize

- Uses the finite-population SD and not the superpopulation SD
- Can show variation decomposition across multiple levels
- Unbalanced data and complex or incomplete designs easily handled
- Can still be done with "fixed" effects

\centerline{
\includegraphics[width=.35\textwidth]{qian1.png}
\includegraphics[width=.35\textwidth]{qian2.png}
}

\vspace{5mm}
\tiny{Hector et al. 2011, Qian and Shen 2007, Gelman 2005}

## Bayesian ANOVA: JAGS

- Compute finite-population SDs computation as derived quantities

\tiny
``` {r, eval=FALSE}
# Priors
mu ~ dnorm(0, 0.001)
for (i in 1:2){
  sigma[i] ~ dunif(0, 100)
  tau[i] <- 1 / ( sigma[i] * sigma[i])
}

# Likelihood
for (i in 1:5){			
  alpha[i] ~ dnorm (0, tau[2])
}
for (i in 1:50) {
  y[i] ~ dnorm(y.hat[i], tau[1]) 
  y.hat[i] <- mu + alpha[x[i]]
  s.yerr[i] <- y[i] - y.hat[i]
}

# Derived quantities
for (i in 1:5){			
  cell[i] <- mu + mean(alpha[i]) 
}
s.alpha <- sd(alpha[]) 
s.y <- sd(y.err[])
```

## Mutiple Comparisons of means

- Fundamentally different approach to mean comparisons
- Shrinkage and/or informed priors

\centerline{\includegraphics[height=2.25in]{GelmanPaper.png}}
Note that is is more "difficult" to find differences among means in the multi-level case because the are pulled together by partial pooling.
\vspace{8mm}
\tiny{Gelman 2013, Gelman et al. 2012}

## Multiple comparisons of cell means

Inference on differences between cell means are calculated directly in a means model or indirectly in an effects model. They are made as the difference posterior distribution of the difference between cell means. These are analogous to single degree of freedom contrasts or Tukeys or the like, but a lot less trouble. But what about the problem of multiple comparisons? 

## Multiple comparisons of cell means

Multiple comparisons are reliable if the model is hierarchical such that means or effects are drawn from a distribution. Illustrating:

$$[\boldsymbol{\mu},\sigma^{2},\boldsymbol{\alpha},\varsigma_{\mu}^{2}\mid\mathbf{y}]\propto\prod_{i=1}^{n_{j}}\prod_{j=1}^{J}[y_{ij}|\mu_{j},\sigma^{2}][\mu_{j}|\boldsymbol{\alpha},\varsigma_{\mu}^{2}][\sigma^{2}][\varsigma^{2}][\alpha]$$
Subtract one cell mean from another to get posterior distribution of difference of means. Shrinkage of the distribution of means as the number of means increases assures that it becomes more difficult for the posterior distribution of a difference between to exclude 0. Neat and tidy.

This also holds for effects models where cell means are calculated from effects and the control or grand mean.


## Futre study
1. Hobbs and Hooten, chapters 6.2.3 and 10.2.

2. A. Gelman, J. B. Carlin, H. S. Stern, D. Dunson, A. Vehhtari, and D. B. Rubin. Bayesian data analysis. 2013 Chapman and Hall / CRC, London, UK.

3. A. Gelman, and J. Hill. 2009. Data analysis using regression and multilevel / hierarchical models. Cambridge University Press, Cambridge, UK. 

4. McCarthy, M. A. 2007. Bayesian Methods for Ecology. Cambridge University Press, Cambridge, U. K. 

## Take home from this exhausing lecture

* Analysis of designed experiments closely resembles other types of Bayesian modeling, providing enormous flexibility to the experimentalist.

* Model types

    -Effects models can be specified analogous to regression except that design matrix is composed of zeros and ones. 
    -Effects models can be specified using subscripts on coefficients without a design matrix
    -Means models estimates the means of treatment cells.

* We can use moment matching and all of the hierarchical tricks we have learned to flexibly create models for analysis of designed experiments:


## Future study
1. Hobbs and Hooten, chapters 6.2.3 and 10.2.

2. A. Gelman, J. B. Carlin, H. S. Stern, D. Dunson, A. Vehhtari, and D. B. Rubin. Bayesian data analysis. 2013 Chapman and Hall / CRC, London, UK.

3. A. Gelman, and J. Hill. 2009. Data analysis using regression and multilevel / hierarchical models. Cambridge University Press, Cambridge, UK. 

4. McCarthy, M. A. 2007. Bayesian Methods for Ecology. Cambridge University Press, Cambridge, U. K. 

## References

\footnotesize

[1] A. Gelman. Analysis of variance – why it is more important than ever. Annals of Statistics, 33(1):1–31, 2005.

[2] A. Gelman and J. Hill. Data analysis using regression and multilevel/hierarchical models. Cambridge University Press, Boston, MA, USA, 2007.

[3] A. Gelman, J. Hill, and M. Yajima. Why we (usually) don’t have to worry about multiple comparisons. Journal of Research on Educational Effectiveness, 5(2):189–211, 2012.

[4] A. Hector, T. Bell, Y. Hautier, F. Isbell, M. K$\'{e}$ry, P. B. Reich, J. van Ruijven, and B. Schmid. BUGS in the analysis of biodiversity experiments: Species richness and composition are of similar importance for grassland productivity. PLoS ONE, 6(3):e17434, 2011.

[5] S. S. Qian and Z. Shen. Ecological applications of multilevel analysis of variance. Ecology, 88(10):2489– 2495, 2007.

[6] A. Gelman and E. Loken. The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Department of Statistics, Columbia University, 2013.

